{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e99b770-d549-4554-8659-0ecaf7fcb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  This notebook demonstrates the following:\n",
    "#  * gemma_2b_en vs gemma_1.1_instruct_2b_en\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d83c265-3ff1-43a7-a154-c374a89ee870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python imports\n",
    "#\n",
    "#  Notes:\n",
    "#  * Make sure you install the packages in requirements.txt\n",
    "#  * Make sure you setup your KAGGLE secrets via env vars.\n",
    "#\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
    "from keras.models import load_model\n",
    "import kagglehub\n",
    "from langchain.schema.runnable import Runnable\n",
    "from typing import Any, Optional\n",
    "import tensorflow as tf\n",
    "from keras.config import disable_interactive_logging\n",
    "import gc\n",
    "from IPython.display import Markdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0824f907-e99e-4e7e-aa3a-39ca61a179be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions we will use later to display the interaction with the model\n",
    "#\n",
    "def display_chat(prompt, text):\n",
    "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
    "  prompt = prompt.replace('\\n','<br>')\n",
    "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  text = textwrap.indent(text, '', predicate=lambda _: True)\n",
    "  text = text.replace('\\n\\n','<br><br>')\n",
    "  text = text.replace('\\n','<br>')\n",
    "  text = text.replace(\"```\",\"\")\n",
    "  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + text + \"</blockquote></font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)\n",
    "def to_markdown(text):\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bfe6692-db18-4b64-ae22-66fbe28062e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default Gemma model, not fine-tuned\n",
    "#\n",
    "model_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "974790d5-249c-45a2-8348-9a9c342d5659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Tell me, in a few words,  how to compute all prime numbers up to 1000?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote><br><br>Answer:<br><br>Step 1/5<br>1. Start by finding the first prime number, which is 2.<br><br>Step 2/5<br>2. Check if 2 is a prime number by dividing it by all numbers less than or equal to 2. If it is not divisible by any number, then it is a prime number.<br><br>Step 3/5<br>3. If 2 is not a prime number, then find the next prime number, which is 3.<br><br>Step 4/5<br>4. Check if 3 is a prime number by dividing it by all numbers less than or equal to 3. If it is not divisible by any number, then it is a prime number.<br><br>Step 5/5<br>5. Continue this process until you reach 1000. Here is a Python code to do this: ``` prime_numbers = [2] for i in range(3, 1000): if i % 2 == 0: continue if i % 3 == 0: continue if i % 5 == 0: continue if i % 7 == 0: continue if i % 11 == 0: continue if i % 13 == 0: continue if i % 17 == 0: continue if i % 19 == 0: continue if i % 23 == 0: continue if i % 29 == 0: continue if i % 31 == 0: continue if i % 37 == 0: continue if i % 41 == 0: continue if i % 43 == 0: continue if i % 47 == 0: continue if i % 53 == 0: continue if i % 59 == 0: continue if i % 61 == 0: continue if i % 67 == 0: continue if i % 71 == 0: continue if i % 73 == 0: continue if i % 79 == 0: continue if i % 83 == 0: continue if i % 89 == 0: continue if i % 97 == 0: continue if i % 101 == 0: continue prime_numbers.append(i) print(prime_numbers) ``` This code will print all prime numbers up to 1000.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a simple math problem\n",
    "#\n",
    "prompt = \"Tell me, in a few words,  how to compute all prime numbers up to 1000?\"\n",
    "completion = model_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "575bda53-3bc3-437b-bf03-bf2749f82ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note above that the default \"gemma_2b_en\" model already has the ability to take requests and commands from the user\n",
    "# and returns a suitable response.  In this case, it answered our question about a simple math problem.  \n",
    "# Now, lets try to ask a follow up question in order to re-fine our original request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a28d265-589c-4d9b-87dc-4c37a9608f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Now in Python! No numpy, please!</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote><br><br><h1>Introduction</h1><br><br>This is a simple implementation of the <strong><em>K-Means</em></strong> clustering algorithm in Python.<br><br><h1>What is K-Means?</h1><br><br>K-Means is a <strong><em>clustering algorithm</em></strong> that groups a set of data points into <strong><em>k</em></strong> clusters.<br><br>The algorithm is based on the <strong><em>inertia</em></strong> of the clusters.<br><br>The <strong><em>inertia</em></strong> is the sum of the squared distances of each data point to its nearest cluster center.<br><br>The goal of the algorithm is to find the <strong><em>k</em></strong> cluster centers that minimize the <strong><em>inertia</em></strong>.<br><br><h1>How does it work?</h1><br><br>The algorithm starts by randomly selecting <strong><em>k</em></strong> cluster centers.<br><br>Then, it iteratively assigns each data point to the cluster center that is closest to it.<br><br>This process is repeated until the <strong><em>inertia</em></strong> does not change significantly.<br><br><h1>Implementation</h1><br><br>The code is available on my GitHub.<br><br><h1>Conclusion</h1><br><br>This is a simple implementation of the K-Means algorithm in Python.<br><br>It is not the most efficient or the most accurate implementation, but it should be enough for most practical applications.<br><br>If you have any questions or suggestions, feel free to leave a comment.<br><br>Thanks for reading!</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a follow-up question\n",
    "#\n",
    "prompt = \"Now in Python! No numpy, please!\"\n",
    "completion = model_lm.generate(prompt)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45cbbf5a-09f6-4ad2-b631-0d75cd67893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the answer to the folow-up question had nothing to do with our original request.\n",
    "# In order to support a more conversational interaction with Gemma, Google has release a version\n",
    "# of the model that's more helpful.  Let's try that one, called \"gemma_1.1_instruct_2b_en\" now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4b46c41-cacb-4a3f-b499-ceffdd7d359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model \"gemma_1.1_instruct_2b_en\" supports adding conversation context into the prompt but it\n",
    "# requires using a specifc set of tags in the prompt.  The following code makes it easy to do this\n",
    "# by encapsulating all of that formatting and it also retains the conversation history with the model.\n",
    "#\n",
    "class ChatState():\n",
    "  \"\"\"\n",
    "  Manages the conversation history for a turn-based chatbot\n",
    "  Follows the turn-based conversation guidelines for the Gemma family of models\n",
    "  documented at https://ai.google.dev/gemma/docs/formatting\n",
    "  \"\"\"\n",
    "  __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
    "  __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
    "  __END_TURN__ = \"<end_of_turn>\\n\"\n",
    "  def __init__(self, model, system=\"\"):\n",
    "    \"\"\"\n",
    "    Initializes the chat state.\n",
    "    Args:\n",
    "        model: The language model to use for generating responses.\n",
    "        system: (Optional) System instructions or bot description.\n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    self.system = system\n",
    "    self.history = []\n",
    "  def add_to_history_as_user(self, message):\n",
    "      \"\"\"\n",
    "      Adds a user message to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
    "  def add_to_history_as_model(self, message):\n",
    "      \"\"\"\n",
    "      Adds a model response to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n",
    "  def get_history(self):\n",
    "      \"\"\"\n",
    "      Returns the entire chat history as a single string.\n",
    "      \"\"\"\n",
    "      return \"\".join([*self.history])\n",
    "  def get_full_prompt(self):\n",
    "    \"\"\"\n",
    "    Builds the prompt for the language model, including history and system description.\n",
    "    \"\"\"\n",
    "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
    "    if len(self.system)>0:\n",
    "      prompt = self.system + \"\\n\" + prompt\n",
    "    return prompt\n",
    "\n",
    "  def send_message(self, message):\n",
    "    \"\"\"\n",
    "    Handles sending a user message and getting a model response.\n",
    "    Args:\n",
    "        message: The user's message.\n",
    "    Returns:\n",
    "        The model's response.\n",
    "    \"\"\"\n",
    "    self.add_to_history_as_user(message)\n",
    "    prompt = self.get_full_prompt()\n",
    "    response = self.model.generate(prompt, max_length=1024)\n",
    "    result = response.replace(prompt, \"\")  # Extract only the new response    \n",
    "    self.add_to_history_as_model(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ca3ea6c-c5d7-42ad-831d-4d77a6f44a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the model and the chat helper code\n",
    "#\n",
    "model_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_2b_en\")\n",
    "chat = ChatState(model_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d070d64-91d5-4859-b5a7-a0c48c8ca7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Tell me, in a few words,  how to compute all prime numbers up to 1000?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>The Sieve of Eratosthenes is a widely used method to compute all prime numbers up to a given limit. It involves iteratively marking out multiples of each prime number.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's repeat the conversation with \"gemma_1.1_instruct_2b_en\" to see if it performs better.\n",
    "# Let's start with the question...\n",
    "#\n",
    "prompt = \"Tell me, in a few words,  how to compute all prime numbers up to 1000?\"\n",
    "response = chat.send_message(prompt)\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d18b0c0-288b-41b5-8c25-4c517267cdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Now in Python! No numpy, please!</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>python<br>def prime(n):<br>    if n <= 1:<br>        return False<br>    for i in range(2, int(n**0.5) + 1):<br>        if n % i == 0:<br>            return False<br>    return True<br></blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's ask the follow-up question...\n",
    "#\n",
    "prompt = \"Now in Python! No numpy, please!\"\n",
    "response = chat.send_message(prompt)\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4662b986-b55a-4e0d-b500-fa1c87e2ad26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release resources that consume lots of memory\n",
    "chat = None\n",
    "model_lm = None\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab9ed3-87f6-4322-9168-23344f503731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
