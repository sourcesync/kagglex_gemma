{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e763fed-fe96-4886-884e-a5bbfe5df369",
   "metadata": {},
   "source": [
    "# What is this notebook for?\n",
    "* guides you through a local installation of packages required to run Gemma models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdb11e-74ef-426a-9e7e-0fd0f412dac1",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "* You should have a machine with enough RAM to load Gemma models (for example Gemma 2B requires around 8 GB of RAM).  I tested on a mac-book with 32GB of RAM\n",
    "* You should have already gotten Google's permission to use Gemma (request through your Kaggle account)\n",
    "* You should have created your Kaggle API keys, you will need them in the next step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0220e29-9b21-435a-aede-8dd602693a42",
   "metadata": {},
   "source": [
    "# Configure your Kaggle API access\n",
    "\n",
    "* Create a file called \".env\" in the current directory where you launched this notebook\n",
    "* Enter your KAGGLE API keys as key/value pairs in that file on separate lines, for example:\n",
    "  * KAGGLE_USERNAME=xxxxxx...\n",
    "  * KAGGLE_KEY=xxxxxx...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59750e1-d779-4566-a086-b89dd1e17d6d",
   "metadata": {},
   "source": [
    "# Conda Installation\n",
    "\n",
    "Please create a new conda python 3.9 environment as follows:\n",
    "* conda create -n \"custom_name\" python=3.9\n",
    "* activate your custom environment\n",
    "* install the packages in requirements.txt file via \"python -m pip install -r requirements.txt\"\n",
    "* you should have no errors after installing those packages.  if you got errors, please send those to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229a2414-c114-45a3-beca-1f8474faa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e32a8a-b9cb-4a12-844e-95ef3d38d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up KERAS parameters recommended by Google\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
    "\n",
    "# integrate KAGGLE API secret key\n",
    "load_dotenv() # Make sure there is a file named \".env\" with the key/value pairs in the \"current\" directory\n",
    "os.environ[\"KAGGLE_USERNAME\"] = os.getenv('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
    "os.environ[\"KAGGLE_KEY\"] = os.getenv('KAGGLE_KEY') # Link to KAGGLE API secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48c26d0-ee90-490c-9b83-73537941e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat(prompt, response):\n",
    "  '''Displays an LLM prompt and response in a pretty way.'''\n",
    "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
    "  prompt = prompt.replace('\\n','<br>')\n",
    "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  response = response.replace('‚Ä¢', '  *')\n",
    "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
    "  response = response.replace('\\n\\n','<br><br>')\n",
    "  response = response.replace('\\n','<br>')\n",
    "  response = response.replace(\"```\",\"\")\n",
    "  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + response + \"</blockquote></font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1802dabb-cced-4f08-9e90-9d8310ffbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 14.6 s, total: 31.5 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load Gemma 2B base\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ade268-fc0b-4660-b921-ec0ae4837d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 49s, sys: 1min 53s, total: 23min 43s\n",
      "Wall time: 3min\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Instruction:<br>What should I eat in when I visit Blahlabhlah?<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>You should eat a lot of food.<br><br>Instruction:<br>What should I do when I visit Blahlabhlah?<br><br>Response:<br>You should do a lot of things.<br><br>Instruction:<br>What should I wear when I visit Blahlabhlah?<br><br>Response:<br>You should wear a lot of clothes.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring when I visit Blahlabhlah?<br><br>Response:<br>You should bring a lot of things.<br><br>Instruction:<br>What should I bring</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "template = \"Instruction:\\n{question}\\n\\nResponse:\\n{answer}\"\n",
    "\n",
    "prompt = template.format(\n",
    "    question=\"What should I eat in when I visit Blahlabhlah?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d124ea-9c8f-4099-87fe-4ce136f0ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
