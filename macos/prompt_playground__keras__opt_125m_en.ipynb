{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89141b96-e60c-4909-91ee-37aaa7383adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  This notebook demonstrates the following:\n",
    "#  * Use the Keras built-in OPT model ('opt_125m_en')\n",
    "#  * Shows various methods to complete a prompt in Keras\n",
    "#    * model generate() method\n",
    "#    * Langchain Runnable class\n",
    "#    * ChatVertexAI\n",
    "#  * Experiments with ways to integrate conversation history\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b68d11-29a0-4c0e-aa52-54b21b16d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python imports\n",
    "#\n",
    "#  Notes:\n",
    "#  * Make sure you install the packages in requirements.txt\n",
    "#  * Make sure you setup your KAGGLE secrets via env vars.\n",
    "#\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import GemmaBackbone, BertBackbone, OPTBackbone\n",
    "from keras.models import load_model\n",
    "import kagglehub\n",
    "from langchain.schema.runnable import Runnable\n",
    "import langchain_core\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing import Any, Optional\n",
    "import tensorflow as tf\n",
    "from keras.config import disable_interactive_logging\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e6c309-c3f4-41c3-8af0-608a4ddc0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Global config\n",
    "#\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\" # Try to suppress annoying GRPC warnings\n",
    "os.environ[\"GLOG_minloglevel\"] = \"3\"   # Try to suppress annoying GLOG warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # Try to suppress annoying TF CPP warnings\n",
    "# Try to suppress general warnings sent through the warnings module\n",
    "def warn(*args, **kwargs): \n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "disable_interactive_logging() # Try to support keras warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85b0f34-f061-458a-9fc4-57f8c24e66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions\n",
    "#\n",
    "def makebold(txt):\n",
    "    return '\\x1b[1;30m'+txt+'\\x1b[0m'\n",
    "def makeblue(txt):\n",
    "    return '\\x1b[1;34m'+txt+'\\x1b[0m'\n",
    "def makegreen(txt):\n",
    "    return '\\x1b[1;32m'+txt+'\\x1b[0m'\n",
    "def makered(txt):\n",
    "    return '\\x1b[1;33m'+txt+'\\x1b[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6427861e-d331-4308-bb78-49d356ced65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initial prompt - I'll use this prompt to contrast-and-compare various \n",
    "# prompt completion techniques using Keras\n",
    "#\n",
    "initial_prompt = \"Tell me a story. My name is Daniel.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a19460-4b94-4b8f-805a-b7d086975f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30mPrompt:\n",
      "\u001b[0m \u001b[1;32m\"Tell me a story. My name is Daniel.\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726947013.688816 11938918 service.cc:146] XLA service 0x600000d98300 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1726947013.688843 11938918 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1726947013.693087 11938921 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m\n",
      "Completion:\n",
      "\u001b[0m \u001b[1;34m\"Tell me a story. My name is Daniel. I am a young woman. I am a student of the University of California, Berkeley. I live in the United States of America. My name is Daniel.\n",
      "\n",
      "I was a student in the University of California, Berkeley when I was born. I was born in a very poor neighborhood. I was born in a very poor neighborhood. I was born a poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor man. He was a very poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor man. He was a very poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58304"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#  Test a basic prompt completion using the model generate() call\n",
    "#\n",
    "model_lm = keras_nlp.models.OPTCausalLM.from_preset(\"opt_125m_en\")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=3, seed=2)\n",
    "model_lm.compile(sampler=sampler)\n",
    "\n",
    "# Create the prompt via manual formatting\n",
    "template = \"{instruction}\" # Note: format may depend on the fine-tuning dataset format\n",
    "simple_prompt = template.format(\n",
    "    instruction=initial_prompt,\n",
    "    response=\"\",\n",
    ")\n",
    "print(makebold(\"Prompt:\\n\"), makegreen(\"\\\"\" + simple_prompt + \"\\\"\") )\n",
    "\n",
    "# Complete the prompt via model generate()\n",
    "model_prompt_completion = model_lm.generate(simple_prompt, max_length=2048) # Note: Using the context size of Gemma as max\n",
    "print(makebold(\"\\nCompletion:\\n\"), makeblue(\"\\\"\"+ model_prompt_completion + \"\\\"\" ))\n",
    "\n",
    "model_lm = None # clean up memory\n",
    "gc.collect() # Run python memory collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26310572-7581-4eb2-9ca9-6200431b9151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30mRunnable prompt:\n",
      "\u001b[0m\u001b[1;32m\"Tell me a story. My name is Daniel.\"\u001b[0m\n",
      "\u001b[1;30m\n",
      "Runnable completion:\n",
      "\u001b[0m\u001b[1;34m\"Tell me a story. My name is Daniel. I am a young woman. I am a student of the University of California, Berkeley. I live in the United States of America. My name is Daniel.\n",
      "\n",
      "I was a student in the University of California, Berkeley when I was born. I was born in a very poor neighborhood. I was born in a very poor neighborhood. I was born a poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor man. He was a very poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor man. He was a very poor boy.\n",
      "\n",
      "My father, my grandfather, was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\n",
      "\n",
      "I was a very poor boy.\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Test a basic prompt completion via a minimal Langchain Runnable\n",
    "#\n",
    "class LMRunnable(Runnable):\n",
    "    def __init__(self, model: keras_nlp.models.OPTCausalLM):\n",
    "        self.model = model\n",
    "\n",
    "    def invoke(self, input: Any, config: Optional[dict] = None) -> str:\n",
    "        prompt = str(input)\n",
    "        output = self.model.generate(prompt, max_length=2048)\n",
    "        return output if isinstance(output,str) else \"Unknown output type.\"\n",
    "\n",
    "# Use the Gemma model as a Langchain Runnable\n",
    "model_lm = keras_nlp.models.OPTCausalLM.from_preset(\"opt_125m_en\")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=3, seed=2)\n",
    "model_lm.compile(sampler=sampler)\n",
    "runnable = LMRunnable(model_lm)\n",
    "\n",
    "# Create the prompt via manual formatting\n",
    "runnable_prompt = f\"{initial_prompt}\" # Note: format may depend on the fine-tuning dataset format\n",
    "print(makebold(\"Runnable prompt:\\n\") +  makegreen(\"\\\"\" + runnable_prompt + \"\\\"\") )\n",
    "\n",
    "# Complete the prompt via Runnable invoke()\n",
    "runnable_prompt_completion = runnable.invoke(runnable_prompt)\n",
    "print(makebold(\"\\nRunnable completion:\\n\") + makeblue(\"\\\"\" + runnable_prompt_completion + \"\\\"\"))\n",
    "\n",
    "runnable = None # release model handle\n",
    "model_lm = None # try to clean up model memory\n",
    "gc.collect() # Run python memory collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e918ef00-4219-466e-ac38-ce9bd4afa922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30mRunnable prompt from template:\n",
      "\u001b[0m\u001b[1;32m\"You are a writer of function.\n",
      "\n",
      "Tell me a story. My name is Daniel.\"\u001b[0m\n",
      "\u001b[1;30m\n",
      "Runnable completion:\n",
      "\u001b[0m\u001b[1;34m\"You are a writer of function.\n",
      "\n",
      "Tell me a story. My name is Daniel. I am a writer of function. I am the author of function and function. I am the author of function and function.\n",
      "\n",
      "Tell me a story. I am a writer of function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell me a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell me a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell me a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell me a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us a story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\n",
      "\n",
      "Tell us about your story.\n",
      "\n",
      "I am the author of function and function.\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58282"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Test a basic prompt completion via a minimal Langchain Runnable and ChatPromptTemplate\n",
    "#\n",
    "class LMRunnable(Runnable):\n",
    "    def __init__(self, model: keras_nlp.models.OPTCausalLM):\n",
    "        self.model = model\n",
    "\n",
    "    def invoke(self, input: Any, config: Optional[dict] = None) -> str: \n",
    "        if not isinstance(input,langchain_core.prompt_values.PromptValue):\n",
    "            raise Exception(\"Unknown input type\")\n",
    "        # Note the implementor of this class needs to know about the template\n",
    "        # in order to form the raw prompt to present to the model\n",
    "        raw_prompt = input.messages[0].content + \"\\n\\n\" + input.messages[1].content\n",
    "        output = self.model.generate(raw_prompt, max_length=2048)\n",
    "        return output \n",
    "\n",
    "# Use the Gemma model as a Langchain Runnable\n",
    "model_lm = keras_nlp.models.OPTCausalLM.from_preset(\"opt_125m_en\")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=3, seed=2)\n",
    "model_lm.compile(sampler=sampler)\n",
    "runnable = LMRunnable(model_lm)\n",
    "\n",
    "# Create the prompt via a ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a writer of function.\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "# Make a prompt using the template just defined\n",
    "runnable_prompt = prompt_template.invoke({\"msgs\": [HumanMessage(content=initial_prompt)]})\n",
    "print( makebold(\"Runnable prompt from template:\\n\") \\\n",
    "      + makegreen( \"\\\"\" + runnable_prompt.messages[0].content + \"\\n\\n\" + runnable_prompt.messages[1].content + \"\\\"\") )\n",
    "\n",
    "# Complete the prompt\n",
    "runnable_prompt_completion = runnable.invoke(runnable_prompt)\n",
    "print(makebold(\"\\nRunnable completion:\\n\") + makeblue(\"\\\"\" + runnable_prompt_completion + \"\\\"\"))\n",
    "\n",
    "runnable = None # release model handle\n",
    "model_lm = None # try to clean up model memory\n",
    "gc.collect() # Run python memory collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3f86c6-1e2c-4395-96ef-3adf4cef1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Test a basic prompt completion using Langchain's ChatVertexAI class\n",
    "# Documentation: TODO\n",
    "#\n",
    "\n",
    "# TODO cvai_model = ChatVertexAI(model_name=\"opt_125m_en\", keras_backend=\"jax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30e2e7-9099-4f12-b084-35cb063c96ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
