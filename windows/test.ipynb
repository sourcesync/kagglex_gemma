{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e763fed-fe96-4886-884e-a5bbfe5df369",
   "metadata": {},
   "source": [
    "# What is this notebook for?\n",
    "* a test notebook to try running Gemma models locally on your windows machine\n",
    "* make sure you've successfully run the installation steps in README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdb11e-74ef-426a-9e7e-0fd0f412dac1",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "* You should have a machine with enough RAM to load Gemma models (for example Gemma 2B requires around 8 GB of RAM).  I tested on a windows laptop with 8GB RAM\n",
    "* You should have already gotten Google's permission to use Gemma (request through your Kaggle account)\n",
    "* You should have created your Kaggle API keys, you will need them in the next step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0220e29-9b21-435a-aede-8dd602693a42",
   "metadata": {},
   "source": [
    "# Configure your Kaggle API access\n",
    "\n",
    "* Create a file called \".env\" in the current directory where you launched this notebook\n",
    "* Enter your KAGGLE API keys as key/value pairs in that file on separate lines, for example:\n",
    "  * KAGGLE_USERNAME=xxxxxx...\n",
    "  * KAGGLE_KEY=xxxxxx...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229a2414-c114-45a3-beca-1f8474faa83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e32a8a-b9cb-4a12-844e-95ef3d38d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up KERAS parameters recommended by Google\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
    "\n",
    "# integrate KAGGLE API secret key\n",
    "load_dotenv() # Make sure there is a file named \".env\" with the key/value pairs in the \"current\" directory\n",
    "os.environ[\"KAGGLE_USERNAME\"] = os.getenv('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
    "os.environ[\"KAGGLE_KEY\"] = os.getenv('KAGGLE_KEY') # Link to KAGGLE API secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48c26d0-ee90-490c-9b83-73537941e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat(prompt, response):\n",
    "  '''Displays an LLM prompt and response in a pretty way.'''\n",
    "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
    "  prompt = prompt.replace('\\n','<br>')\n",
    "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  response = response.replace('‚Ä¢', '  *')\n",
    "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
    "  response = response.replace('\\n\\n','<br><br>')\n",
    "  response = response.replace('\\n','<br>')\n",
    "  response = response.replace(\"```\",\"\")\n",
    "  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + response + \"</blockquote></font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802dabb-cced-4f08-9e90-9d8310ffbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_2b_en/1/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 782/782 [00:00<00:00, 574kB/s]\n",
      "2024-10-05 16:58:16.912128: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "2024-10-05 16:58:20.599016: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "2024-10-05 16:58:26.833785: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load Gemma 2B base\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ade268-fc0b-4660-b921-ec0ae4837d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "template = \"Instruction:\\n{question}\\n\\nResponse:\\n{answer}\"\n",
    "\n",
    "prompt = template.format(\n",
    "    question=\"What should I eat in when I visit Blahlabhlah?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
