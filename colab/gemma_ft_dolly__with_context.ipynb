{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcesync/kagglex_gemma/blob/gw%2Finitial/colab/gemma_ft_dolly__with_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  This notebook demonstrates the following:\n",
        "   * fine tuning \"gemma2_2b_en\" on the dolly dataset\n",
        "   * shows prompt completion before and after fine-tuning this model\n",
        "   * it runs successfully in COLAB"
      ],
      "metadata": {
        "id": "QGephukVL9Ih"
      },
      "id": "QGephukVL9Ih"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Get access to Gemma via your Kaggle account:\n",
        "  * Log into your Kaggle account\n",
        "  * Request access to Gemma models using your Kaggle account.  You can follow these instructions here: https://www.kaggle.com/code/nilaychauhan/get-started-with-gemma-using-kerasnlp\n",
        "  * You need to wait for confirmation.  Note that this didn't take too long for me.\n",
        "  * Create an API key in your Kaggle account you will need later.  You can follow these instructions here: https://christianjmills.com/posts/kaggle-obtain-api-key-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "p3JTh7rWMKOv"
      },
      "id": "p3JTh7rWMKOv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure your Colab account can access Gemma:\n",
        "  * Add the Kaggle API key into your COLAB secrets.  You can follow these instructions here: https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0\n",
        "\n"
      ],
      "metadata": {
        "id": "zrWQ3nyEMiEE"
      },
      "id": "zrWQ3nyEMiEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select an AI hardware accelerator\n",
        "  * Select hardware options near the top right of your Colab notebook\n",
        "  * I tested with A100 and it worked well.  Note that I have a Colab Pro subscription.\n"
      ],
      "metadata": {
        "id": "HeSHIKfXMtYF"
      },
      "id": "HeSHIKfXMtYF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required python packages"
      ],
      "metadata": {
        "id": "sA5tc0WbNasG"
      },
      "id": "sA5tc0WbNasG"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\""
      ],
      "metadata": {
        "id": "vG9X2r8yNIIh",
        "outputId": "2453bc93-f542-4825-cc16-0279ea1ce047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vG9X2r8yNIIh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCPU times: user 62.1 ms, sys: 8.27 ms, total: 70.4 ms\n",
            "Wall time: 8.64 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required python packages"
      ],
      "metadata": {
        "id": "7wYTJ1HMNjYK"
      },
      "id": "7wYTJ1HMNjYK"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "47ec2212-9c95-4a99-aede-ae4f86d0b82b",
      "metadata": {
        "id": "47ec2212-9c95-4a99-aede-ae4f86d0b82b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
        "from keras.models import load_model\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "from google.colab import userdata\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure this notebook\n",
        "* set up KERAS parameters recommended by Google\n",
        "* integrate KAGGLE API secret key"
      ],
      "metadata": {
        "id": "KoWM5VdCO5FH"
      },
      "id": "KoWM5VdCO5FH"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY') # Link to KAGGLE API secret key"
      ],
      "metadata": {
        "id": "e1dF1v8EsJaE"
      },
      "id": "e1dF1v8EsJaE",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the fine-tuning dataset"
      ],
      "metadata": {
        "id": "N3HWIrRKPfFQ"
      },
      "id": "N3HWIrRKPfFQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EITskbJJihBY",
        "outputId": "354f259a-12f4-4c69-9705-c3ea8d77aec3"
      },
      "id": "EITskbJJihBY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-27 16:41:07--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.77, 13.35.210.114, 13.35.210.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1727714467&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzcxNDQ2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=sARaaYmVyNnAaeWvoDKHz29E5wkl9DgHlCeHponyvPae%7ESzjhy4JwvjW57sVCwC-dYM6jaZ6owc4qdkcJ64WGHGHRnTLe2lY6qYMS4eFqQTreCTmzvCxuvYCyy6-JONVno3gWH31GvAplI8gag37fgF70nwIFqovgQAccyHdrKohRn6pVENr2gYgU2-JNq6tFUDagKuIdGZjTAA-xW1cfqBcwscM%7EiYfSCMrvYOibpArLAQaVVVfeZhmnmnANReswsSRfIqlbaThFBhDOTF9v-isiDGEbJMl95lqkSmByJIH44hPQ9k%7EHpqCLDMOoVKCSMjgG2rcDto0w5ykNHdzAg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-09-27 16:41:07--  https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1727714467&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzcxNDQ2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=sARaaYmVyNnAaeWvoDKHz29E5wkl9DgHlCeHponyvPae%7ESzjhy4JwvjW57sVCwC-dYM6jaZ6owc4qdkcJ64WGHGHRnTLe2lY6qYMS4eFqQTreCTmzvCxuvYCyy6-JONVno3gWH31GvAplI8gag37fgF70nwIFqovgQAccyHdrKohRn6pVENr2gYgU2-JNq6tFUDagKuIdGZjTAA-xW1cfqBcwscM%7EiYfSCMrvYOibpArLAQaVVVfeZhmnmnANReswsSRfIqlbaThFBhDOTF9v-isiDGEbJMl95lqkSmByJIH44hPQ9k%7EHpqCLDMOoVKCSMjgG2rcDto0w5ykNHdzAg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.37, 18.155.68.85, 18.155.68.87, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13085339 (12M) [text/plain]\n",
            "Saving to: ‘databricks-dolly-15k.jsonl’\n",
            "\n",
            "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-09-27 16:41:07 (239 MB/s) - ‘databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n",
            "\n",
            "/content\n",
            "databricks-dolly-15k.jsonl  sample_data\n",
            "CPU times: user 25.2 ms, sys: 2.32 ms, total: 27.5 ms\n",
            "Wall time: 919 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some useful functions used later\n",
        "* display_chat() function"
      ],
      "metadata": {
        "id": "WeZVI33rQCRP"
      },
      "id": "WeZVI33rQCRP"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fab0264d-bc3a-47e2-8ad2-fc91f9a63a7f",
      "metadata": {
        "id": "fab0264d-bc3a-47e2-8ad2-fc91f9a63a7f"
      },
      "outputs": [],
      "source": [
        "def display_chat(prompt, response):\n",
        "  '''Displays an LLM prompt and response in a pretty way.'''\n",
        "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
        "  prompt = prompt.replace('\\n','<br>')\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  response = response.replace('•', '  *')\n",
        "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
        "  response = response.replace('\\n\\n','<br><br>')\n",
        "  response = response.replace('\\n','<br>')\n",
        "  response = response.replace(\"```\",\"\")\n",
        "  formatted_text = \"<font size='+1' color='teal'>🤖<blockquote>\" + response + \"</blockquote></font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the fine-tuning dataset\n",
        "* loads the dataset into an array for use later\n",
        "* it also formats each record into a specific string format"
      ],
      "metadata": {
        "id": "RDkvoFJ4QQKx"
      },
      "id": "RDkvoFJ4QQKx"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cbedc30a-3c59-47b0-9098-cf245afce06e",
      "metadata": {
        "id": "cbedc30a-3c59-47b0-9098-cf245afce06e"
      },
      "outputs": [],
      "source": [
        "ft_data = []\n",
        "with open(\"/content/databricks-dolly-15k.jsonl\") as file: # For COLAB\n",
        "    for line in file:\n",
        "        features = json.loads(line)\n",
        "        # Filter out examples with context, to keep it simple.\n",
        "        if features[\"context\"]:\n",
        "            continue\n",
        "        # Format the entire example as a single string.\n",
        "        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "        ft_data.append(template.format(**features))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decide on how much fine-tuning data to use\n",
        "* Often this is determined experimentally\n",
        "* I've found at least 1000 data points suffice in general"
      ],
      "metadata": {
        "id": "kBM7_4STRAyH"
      },
      "id": "kBM7_4STRAyH"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "69498cf3-20af-4dd5-abe4-d0ac432ff73b",
      "metadata": {
        "id": "69498cf3-20af-4dd5-abe4-d0ac432ff73b"
      },
      "outputs": [],
      "source": [
        "data = ft_data[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Gemma model"
      ],
      "metadata": {
        "id": "aKx7ylr8RSBL"
      },
      "id": "aKx7ylr8RSBL"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2ed12d69-cec3-4fca-90a1-5c57b7aa4f4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ed12d69-cec3-4fca-90a1-5c57b7aa4f4d",
        "outputId": "198e14f3-51a4-4f9a-d166-c503e939aa2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.6 s, sys: 9.7 s, total: 20.3 s\n",
            "Wall time: 51.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
        "# uncomment the following lines to \"sample the softmax probabilities of the model\"\n",
        "#sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "#gemma_lm.compile(sampler=sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have the model complete a \"raw\" prompt with no formatting\n"
      ],
      "metadata": {
        "id": "-BhDEjMoRoOk"
      },
      "id": "-BhDEjMoRoOk"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "85799e29-8acd-4da3-830a-e9c2791969fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85799e29-8acd-4da3-830a-e9c2791969fe",
        "outputId": "1f8fdeef-fcc5-4509-96c8-08f98e9051fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 12s, sys: 1.61 s, total: 2min 13s\n",
            "Wall time: 1min 2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>What should I do on a trip to Europe?</blockquote></font><font size='+1' color='teal'>🤖<blockquote><br><br>[User 0001]<br><br>I'm going to Europe for the first time in a few months. I'm going to be in Paris, London, and Amsterdam. I'm not sure what to do. I'm not a big fan of museums, but I'm not sure what else to do. I'm not a big fan of shopping, but I'm not sure what else to do. I'm not a big fan of nightlife, but I'm not sure what else to do. I'm not a big fan of food, but I'm not sure what else to do. I'm not a big fan of sports, but I'm not sure what else to do. I'm not a big fan of music, but I'm not sure what else to do. I'm not a big fan of movies, but I'm not sure what else to do. I'm not a big fan of TV, but I'm not sure what else to do. I'm not a big fan of books, but I'm not sure what else to do. I'm not a big fan of video games, but I'm not sure what else to do. I'm not a big fan of computers, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"What should I do on a trip to Europe?\"\n",
        "completion = gemma_lm.generate(prompt,max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have the model complete a formatted prompt"
      ],
      "metadata": {
        "id": "7aJm-g7RVX4x"
      },
      "id": "7aJm-g7RVX4x"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "34e449d9-29eb-4ff7-9168-cab061ab5d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "34e449d9-29eb-4ff7-9168-cab061ab5d38",
        "outputId": "1359b56f-4199-4e07-9b7b-f8daa7b9f16e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Instruction:<br>What should I do on a trip to Europe?<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>🤖<blockquote>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Ask a simple query this time using a specific template per the documentation\n",
        "#\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "completion = gemma_lm.generate(prompt)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable the model for fine-tuning"
      ],
      "metadata": {
        "id": "xPHR2jq4XmYZ"
      },
      "id": "xPHR2jq4XmYZ"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c8f278c2-9f83-4356-a8cb-576100e2f69f",
      "metadata": {
        "id": "c8f278c2-9f83-4356-a8cb-576100e2f69f"
      },
      "outputs": [],
      "source": [
        "gemma_lm.backbone.enable_lora(rank=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model"
      ],
      "metadata": {
        "id": "CzlT-xncXu-O"
      },
      "id": "CzlT-xncXu-O"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5855d1c6-56d4-4606-be21-12824dd6762c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5855d1c6-56d4-4606-be21-12824dd6762c",
        "outputId": "822e666a-da21-4bf8-db21-5ee754a4bcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 59ms/step - loss: 0.8368 - sparse_categorical_accuracy: 0.5382\n",
            "CPU times: user 4min 42s, sys: 11.7 s, total: 4min 54s\n",
            "Wall time: 2min 42s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79c8c0122da0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have the fine-tuned model complete a \"raw\" prompt"
      ],
      "metadata": {
        "id": "nJnzhLqpYNK1"
      },
      "id": "nJnzhLqpYNK1"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"What should I do on a trip to Europe?\"\n",
        "completion = gemma_lm.generate(prompt,max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "id": "i4MZDM-wYZLe",
        "outputId": "5ef01110-df13-4523-ed76-9001ff0b810b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "i4MZDM-wYZLe",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 49.8 s, sys: 1.08 s, total: 50.9 s\n",
            "Wall time: 51.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>What should I do on a trip to Europe?</blockquote></font><font size='+1' color='teal'>🤖<blockquote><br><br>[User 0001]<br><br>I'm going to Europe for the first time in a few months. I'm going to be in Paris, Amsterdam, and London. I'm not sure what to do in each city. I'm looking for some suggestions.<br><br>[User 0002]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0003]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0004]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0005]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0006]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0007]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0008]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0009]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0010]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0011]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0012]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0013]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0014]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0015]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0016]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0017]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0018]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0019]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0020]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0021]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0022]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0023]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0024]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0025]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0026]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0027]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0028]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0029]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0030]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0031]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0032]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0033]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0034]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[User 0035]<br><br>I'm not sure what you mean by \"what to do\" in each city.<br><br>[</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have the fine-tuned model complete a formatted prompt"
      ],
      "metadata": {
        "id": "imdfjX2IYdn6"
      },
      "id": "imdfjX2IYdn6"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f8212850-9613-489c-804f-c127a6329c94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "f8212850-9613-489c-804f-c127a6329c94",
        "outputId": "5e528e7c-1e59-46b6-ca57-863a5de0d573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 10s, sys: 643 ms, total: 1min 11s\n",
            "Wall time: 41 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Instruction:<br>What should I do on a trip to Europe?<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>🤖<blockquote>You should take a train to Paris, France.</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "%%time\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Europe?\",\n",
        "    response=\"\",\n",
        ")\n",
        "completion = gemma_lm.generate(prompt)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
