{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcesync/kagglex_gemma/blob/gw%2Finitial/colab/gemma_ft_dolly__with_context_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  This notebook demonstrates the following:\n",
        "   * fine tuning \"gemma2_2b_en\" on the dolly dataset\n",
        "   * shows prompt completion before and after fine-tuning this model\n",
        "   * it runs successfully in COLAB"
      ],
      "metadata": {
        "id": "QGephukVL9Ih"
      },
      "id": "QGephukVL9Ih"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Get access to Gemma via your Kaggle account:\n",
        "  * Log into your Kaggle account\n",
        "  * Request access to Gemma models using your Kaggle account.  You can follow these instructions here: https://www.kaggle.com/code/nilaychauhan/get-started-with-gemma-using-kerasnlp\n",
        "  * You need to wait for confirmation.  Note that this didn't take too long for me.\n",
        "  * Create an API key in your Kaggle account you will need later.  You can follow these instructions here: https://christianjmills.com/posts/kaggle-obtain-api-key-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "p3JTh7rWMKOv"
      },
      "id": "p3JTh7rWMKOv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure your Colab account can access Gemma:\n",
        "  * Add the Kaggle API key into your COLAB secrets.  You can follow these instructions here: https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0\n",
        "\n"
      ],
      "metadata": {
        "id": "zrWQ3nyEMiEE"
      },
      "id": "zrWQ3nyEMiEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select an AI hardware accelerator\n",
        "  * Select hardware options near the top right of your Colab notebook\n",
        "  * I tested with A100 and it worked well.  Note that I have a Colab Pro subscription.\n"
      ],
      "metadata": {
        "id": "HeSHIKfXMtYF"
      },
      "id": "HeSHIKfXMtYF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required python packages"
      ],
      "metadata": {
        "id": "sA5tc0WbNasG"
      },
      "id": "sA5tc0WbNasG"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\""
      ],
      "metadata": {
        "id": "vG9X2r8yNIIh",
        "outputId": "c848a990-ae4a-466b-e0d2-98b55db58fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vG9X2r8yNIIh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/548.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCPU times: user 35.3 ms, sys: 15.2 ms, total: 50.5 ms\n",
            "Wall time: 7.73 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required python packages"
      ],
      "metadata": {
        "id": "7wYTJ1HMNjYK"
      },
      "id": "7wYTJ1HMNjYK"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "47ec2212-9c95-4a99-aede-ae4f86d0b82b",
      "metadata": {
        "id": "47ec2212-9c95-4a99-aede-ae4f86d0b82b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
        "from keras.models import load_model\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import random\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure this notebook\n",
        "* set up KERAS parameters recommended by Google\n",
        "* integrate KAGGLE API secret key"
      ],
      "metadata": {
        "id": "KoWM5VdCO5FH"
      },
      "id": "KoWM5VdCO5FH"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY') # Link to KAGGLE API secret key"
      ],
      "metadata": {
        "id": "e1dF1v8EsJaE"
      },
      "id": "e1dF1v8EsJaE",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the fine-tuning dataset"
      ],
      "metadata": {
        "id": "N3HWIrRKPfFQ"
      },
      "id": "N3HWIrRKPfFQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EITskbJJihBY",
        "outputId": "37d5aab8-583d-43f2-dec7-cf0daa70da54"
      },
      "id": "EITskbJJihBY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-27 17:15:40--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.114, 13.35.210.66, 13.35.210.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1727716541&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzcxNjU0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=LEf-CyjKSn498QUOoKhAzblGM04O1rGqwgZNkwEI5vWD8gvR-MSX34nbmxG12cOn5muCgy4jstVwk9X9zosINo4lI4QIpkkvU2NK2hZeH8SUhAzSSxNUzr12utNvSmV1MBWjvJbsp15yMyYTYsay%7E-axLaYaZYnv2eprdrSyKMtAryoAXACqVrmrryNUIu33dZe9XQ-oW-XJAayupcPEC8bEGJrudCj6kC96wlir0Xl5awn6FsiKfiKSBkfQTETeaxR8ZbSaUn6v95AR2t7r0oZOiFoeXGeCDOZb7BnB1JsSSp-TRuQhdAVSpmqyPrbt2E7k1nNH7ReEt700EVizJw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-09-27 17:15:41--  https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1727716541&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzcxNjU0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=LEf-CyjKSn498QUOoKhAzblGM04O1rGqwgZNkwEI5vWD8gvR-MSX34nbmxG12cOn5muCgy4jstVwk9X9zosINo4lI4QIpkkvU2NK2hZeH8SUhAzSSxNUzr12utNvSmV1MBWjvJbsp15yMyYTYsay%7E-axLaYaZYnv2eprdrSyKMtAryoAXACqVrmrryNUIu33dZe9XQ-oW-XJAayupcPEC8bEGJrudCj6kC96wlir0Xl5awn6FsiKfiKSBkfQTETeaxR8ZbSaUn6v95AR2t7r0oZOiFoeXGeCDOZb7BnB1JsSSp-TRuQhdAVSpmqyPrbt2E7k1nNH7ReEt700EVizJw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.87, 18.155.68.34, 18.155.68.37, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13085339 (12M) [text/plain]\n",
            "Saving to: ‚Äòdatabricks-dolly-15k.jsonl‚Äô\n",
            "\n",
            "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-09-27 17:15:41 (346 MB/s) - ‚Äòdatabricks-dolly-15k.jsonl‚Äô saved [13085339/13085339]\n",
            "\n",
            "/content\n",
            "databricks-dolly-15k.jsonl  sample_data\n",
            "CPU times: user 15.5 ms, sys: 3.72 ms, total: 19.2 ms\n",
            "Wall time: 915 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some useful functions used later\n",
        "* display_chat() function"
      ],
      "metadata": {
        "id": "WeZVI33rQCRP"
      },
      "id": "WeZVI33rQCRP"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fab0264d-bc3a-47e2-8ad2-fc91f9a63a7f",
      "metadata": {
        "id": "fab0264d-bc3a-47e2-8ad2-fc91f9a63a7f"
      },
      "outputs": [],
      "source": [
        "def display_chat(prompt, response):\n",
        "  '''Displays an LLM prompt and response in a pretty way.'''\n",
        "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
        "  prompt = prompt.replace('\\n','<br>')\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  response = response.replace('‚Ä¢', '  *')\n",
        "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
        "  response = response.replace('\\n\\n','<br><br>')\n",
        "  response = response.replace('\\n','<br>')\n",
        "  response = response.replace(\"```\",\"\")\n",
        "  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + response + \"</blockquote></font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the fine-tuning dataset and sample a record\n",
        "* loads the dataset into an array\n",
        "* randomly sample a record for use later"
      ],
      "metadata": {
        "id": "RDkvoFJ4QQKx"
      },
      "id": "RDkvoFJ4QQKx"
    },
    {
      "cell_type": "code",
      "source": [
        "ft_dataset_all = []\n",
        "with open(\"/content/databricks-dolly-15k.jsonl\") as file:\n",
        "    ft_dataset_all = [ json.loads(ln) for ln in file.readlines()]\n",
        "ft_record = random.choice(ft_dataset_all)\n",
        "pprint.pp(ft_record)"
      ],
      "metadata": {
        "id": "ijNavgkwfMQg",
        "outputId": "66475015-f0d9-4891-84e2-42b0ac9aea88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ijNavgkwfMQg",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'As per the passage which languages did Ram Mohan Roy know?',\n",
            " 'context': 'Ram Mohan Roy was born in\\xa0Radhanagar,\\xa0Hooghly District,\\xa0'\n",
            "            'Bengal Presidency. His great grandfather Krishnakanta '\n",
            "            'Bandyopadhyay was a Rarhi\\xa0Kulin\\xa0(noble)\\xa0Brahmin. Among '\n",
            "            'Kulin Brahmins\\xa0‚Äì descendants of the six families of Brahmins '\n",
            "            'imported from\\xa0Kannauj\\xa0by\\xa0Ballal Sen\\xa0in the 12th '\n",
            "            'century\\xa0‚Äì those from the Rarhi district of West Bengal were '\n",
            "            'notorious in the 19th century for living off dowries by marrying '\n",
            "            'several women.\\xa0Kulinism\\xa0was a synonym for polygamy and the '\n",
            "            'dowry system, both of which Rammohan campaigned against.\\xa0His '\n",
            "            'father, Ramkanta, was a\\xa0Vaishnavite, while his mother, Tarini '\n",
            "            'Devi, was from a\\xa0Shaivite\\xa0family. He was a great scholar of '\n",
            "            'Sanskrit, Persian and English languages and also knew Arabic, '\n",
            "            'Latin and Greek. One parent prepared him for the occupation of a '\n",
            "            'scholar, the\\xa0Shastri, while the other secured for him all the '\n",
            "            'worldly advantages needed to launch a career in the\\xa0laukik\\xa0'\n",
            "            'or worldly sphere of public administration.[citation needed]\\xa0'\n",
            "            'Torn between these two parental ideals from early childhood, Ram '\n",
            "            'Mohan vacillated between the two for the rest of his life.During '\n",
            "            'his childhood Ram Mohan Roy witnessed death of his sister in law '\n",
            "            'through\\xa0sati. The seventeen year old girl was dragged towards '\n",
            "            'the pyre where Ram Mohan Roy witnessed her terrified state. He '\n",
            "            'tried to protest but to no avail. She was burned alive. The '\n",
            "            'people chanted \"Maha Sati! Maha Sati! Maha Sati!\" (great wife) '\n",
            "            'over her painful screams.\\xa0',\n",
            " 'response': 'Sanskrit, Persian, English, Arabic, Latin and Greek.',\n",
            " 'category': 'information_extraction'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decide on how much fine-tuning data to use\n",
        "* Often this is determined experimentally\n",
        "* I've found at least 1000 data points suffice in general"
      ],
      "metadata": {
        "id": "kBM7_4STRAyH"
      },
      "id": "kBM7_4STRAyH"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "69498cf3-20af-4dd5-abe4-d0ac432ff73b",
      "metadata": {
        "id": "69498cf3-20af-4dd5-abe4-d0ac432ff73b"
      },
      "outputs": [],
      "source": [
        "ft_data = ft_dataset_all[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Gemma model"
      ],
      "metadata": {
        "id": "aKx7ylr8RSBL"
      },
      "id": "aKx7ylr8RSBL"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2ed12d69-cec3-4fca-90a1-5c57b7aa4f4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ed12d69-cec3-4fca-90a1-5c57b7aa4f4d",
        "outputId": "b27f1140-17aa-4052-e491-44330a61cfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.01 s, sys: 8.9 s, total: 17.9 s\n",
            "Wall time: 46.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
        "# uncomment the following lines to \"sample the softmax probabilities of the model\"\n",
        "#sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "#gemma_lm.compile(sampler=sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask the model something related to the random record\n",
        "* If its not general knowledge (or in the pre-training of Gemma), we would not expect the model to \"know\" anything about it."
      ],
      "metadata": {
        "id": "Fdy0sG8Oi0Q2"
      },
      "id": "Fdy0sG8Oi0Q2"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "85799e29-8acd-4da3-830a-e9c2791969fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85799e29-8acd-4da3-830a-e9c2791969fe",
        "outputId": "0bbc9fc2-7e4a-4124-d803-022907fde154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 11s, sys: 1.36 s, total: 2min 12s\n",
            "Wall time: 1min 2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Who is Ram Mohan Roy?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote><br><br>Ram Mohan Roy was born in 1772 in the village of Radhanagar, in the district of Hooghly, in the province of Bengal, in the Indian subcontinent. He was the son of a Brahmin priest.<br><br>He was a Hindu reformer, who was born in a Brahmin family. He was a great scholar and a great thinker. He was a great reformer and a great social reformer. He was a great reformer of the Hindu religion. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was a great reformer of the Hindu society. He was</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"Who is Ram Mohan Roy?\"\n",
        "completion = gemma_lm.generate(prompt,max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model seems to know something!  Let's ask it a specific question in the random record context."
      ],
      "metadata": {
        "id": "zie28wP9j6L6"
      },
      "id": "zie28wP9j6L6"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"What languages did Ram Mohan Roy know?\"\n",
        "completion = gemma_lm.generate(prompt,max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "id": "fuYmAfLYkRAR",
        "outputId": "fa0522e3-f5a8-4a69-84eb-21fc71569c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "fuYmAfLYkRAR",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.6 s, sys: 1.25 ms, total: 12.6 s\n",
            "Wall time: 12.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>What languages did Ram Mohan Roy know?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote><br><br>[Answer 1]<br><br>He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and Urdu.<br><br>    He was a polyglot. He knew Bengali, English, Persian, Arabic, Sanskrit, and</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# At this point, the model's answer has some issues:\n",
        "* The responses aren't well-formmatted\n",
        "* The response is wrong (if we are to believe the context data is the truth).\n",
        "* Let's try some fine-tuning to fix this."
      ],
      "metadata": {
        "id": "yFqA9HU_kwes"
      },
      "id": "yFqA9HU_kwes"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First, let's prepare a fine-tuning dataset to deal with the response formatting issue (so-called \"instruction following\")\n",
        "* we won't use the context field"
      ],
      "metadata": {
        "id": "ZIuepz4MlFrL"
      },
      "id": "ZIuepz4MlFrL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)# New Section"
      ],
      "metadata": {
        "id": "bH_I9KQlmj38"
      },
      "id": "bH_I9KQlmj38"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cbedc30a-3c59-47b0-9098-cf245afce06e",
      "metadata": {
        "id": "cbedc30a-3c59-47b0-9098-cf245afce06e",
        "outputId": "8cbb7eb4-4f59-4afd-8928-50e75e9c7d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Instruction:\\n'\n",
            " 'tell me whether these are synonyms or antonyms of love: dislike, care, like, '\n",
            " 'hate, affection, harsh\\n'\n",
            " '\\n'\n",
            " 'Response:\\n'\n",
            " 'Synonyms: care, like, affection\\n'\n",
            " 'Antonyms: dislike, hate, harsh')\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for item in ft_data:\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "    data.append(template.format(**item))\n",
        "pprint.pp(random.choice(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model for just proper \"instruction following\""
      ],
      "metadata": {
        "id": "U8Rg_kf5m2A2"
      },
      "id": "U8Rg_kf5m2A2"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "\n",
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ],
      "metadata": {
        "id": "-1to848bm7oH",
        "outputId": "d89a3ebc-aa53-4061-e9ea-22354adbbd26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-1to848bm7oH",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 58ms/step - loss: 0.8256 - sparse_categorical_accuracy: 0.5422\n",
            "CPU times: user 4min 40s, sys: 11.7 s, total: 4min 52s\n",
            "Wall time: 2min 40s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a1a26042800>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's ask the fine-tuned model the same question\n",
        "* we should expect better response formatting (ie, instruction following)\n",
        "* we should not expect it answering correctly based on the fine-tuning context data since we did not use it in the fine-tuning dataset"
      ],
      "metadata": {
        "id": "7aJm-g7RVX4x"
      },
      "id": "7aJm-g7RVX4x"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "34e449d9-29eb-4ff7-9168-cab061ab5d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "34e449d9-29eb-4ff7-9168-cab061ab5d38",
        "outputId": "68a22db3-a53d-4ec9-e899-35e069f273bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Instruction:<br>What languages did Ram Mohan Roy know?<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>Ram Mohan Roy was a Bengali polymath who was a pioneer of the Indian Renaissance. He was a scholar of Sanskrit, Persian, Arabic, and English.</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What languages did Ram Mohan Roy know?\",\n",
        "    response=\"\",\n",
        ")\n",
        "completion = gemma_lm.generate(prompt)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's create a fine-tuning dataset using the dataset context\n",
        "* in this case, we assume the dataset context has ground-truth facts that the model should be using"
      ],
      "metadata": {
        "id": "O9wKJzq4o5T3"
      },
      "id": "O9wKJzq4o5T3"
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for item in ft_data:\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nResponse:\\n{response}\"\n",
        "    data.append(template.format(**item))\n",
        "pprint.pp(random.choice(data))"
      ],
      "metadata": {
        "id": "Prso5XQypNvU",
        "outputId": "10b86c4d-72e4-48f3-9666-e0ebaaa2e214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Prso5XQypNvU",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Instruction:\\n'\n",
            " 'Given a reference text about Georg Friedrich Parrot, tell me when and where '\n",
            " 'he was born as well as what he studied.\\n'\n",
            " '\\n'\n",
            " 'Context:\\n'\n",
            " 'Georg Friedrich Parrot (15 July 1767 ‚Äì 8 July 1852) was a German scientist, '\n",
            " 'the first rector of the Imperial University of Dorpat (today Tartu, Estonia) '\n",
            " 'in what was then the Governorate of Livonia of the Russian Empire.\\n'\n",
            " '\\n'\n",
            " 'Education\\n'\n",
            " 'Georges-Fr√©d√©ric Parrot was born in M√∂mpelgard (now Montb√©liard) (then part '\n",
            " 'of the Duchy of W√ºrttemberg, from 1806 in France). His father, a surgeon by '\n",
            " \"profession and the local duke's physician in ordinary, had a respectable \"\n",
            " 'position in the society becoming the mayor of his hometown. As the family '\n",
            " 'was Protestants, they sent Georg Friedrich to study physics and mathematics '\n",
            " 'at the University of Stuttgart in Stuttgart, the capital of the Duchy '\n",
            " '(1782‚Äì1786).\\n'\n",
            " '\\n'\n",
            " 'Response:\\n'\n",
            " 'Georg Friedrich Parrot was born on July 15, 1767 in M√∂mpelgard. He studied '\n",
            " 'physics and mathematics at the University of Stuttgart.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload base model and fine-tune on the new dataset which has context included"
      ],
      "metadata": {
        "id": "xPHR2jq4XmYZ"
      },
      "id": "xPHR2jq4XmYZ"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c8f278c2-9f83-4356-a8cb-576100e2f69f",
      "metadata": {
        "id": "c8f278c2-9f83-4356-a8cb-576100e2f69f",
        "outputId": "d9972f5e-9a54-4b0b-fccb-5ec9d6f343fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 59ms/step - loss: 1.0938 - sparse_categorical_accuracy: 0.5655\n",
            "CPU times: user 2min 22s, sys: 19.5 s, total: 2min 42s\n",
            "Wall time: 3min 6s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a19f0c0e290>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "%%time\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "\n",
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 256\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's ask the new fine-tuned model the same question\n",
        "* we should expect better response formatting (ie, instruction following)\n",
        "* we might expect it to snwer correctly based on the fine-tuning context data since we used it in the fine-tuning dataset"
      ],
      "metadata": {
        "id": "JdKBB7Xgq8-Q"
      },
      "id": "JdKBB7Xgq8-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What languages did Ram Mohan Roy know?\",\n",
        "    context=\"\",\n",
        "    response=\"\",\n",
        ")\n",
        "completion = gemma_lm.generate(prompt)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "id": "0MC1AIjlq-A4",
        "outputId": "7b11074e-d42a-490d-851e-861860c5e73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "id": "0MC1AIjlq-A4",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Instruction:<br>What languages did Ram Mohan Roy know?<br><br>Context:<br><br><br>Response:<br></blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>Ram Mohan Roy was a Bengali polymath who was a pioneer of the Indian Renaissance. He was a scholar of Sanskrit, Persian, Arabic, and English. He was also a poet, a translator, a journalist, a social reformer, and a politician.</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with the context \"fact\" did not seem to help.  Let's just prompt the model with the context at prompt-time\n",
        "\n"
      ],
      "metadata": {
        "id": "y_Qtv3l_rxMM"
      },
      "id": "y_Qtv3l_rxMM"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"What languages did Ram Mohan Roy know?\",\n",
        "    context=ft_record['context'],\n",
        "    response=\"\",\n",
        ")\n",
        "completion = gemma_lm.generate(prompt)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "id": "slQ_U9kCr83K",
        "outputId": "7e979bd7-c835-4397-ee88-16b2da6bd7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "id": "slQ_U9kCr83K",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Instruction:<br>What languages did Ram Mohan Roy know?<br><br>Context:<br>Ram Mohan Roy was born in¬†Radhanagar,¬†Hooghly District,¬†Bengal Presidency. His great grandfather Krishnakanta Bandyopadhyay was a Rarhi¬†Kulin¬†(noble)¬†Brahmin. Among Kulin Brahmins¬†‚Äì descendants of the six families of Brahmins imported from¬†Kannauj¬†by¬†Ballal Sen¬†in the 12th century¬†‚Äì those from the Rarhi district of West Bengal were notorious in the 19th century for living off dowries by marrying several women.¬†Kulinism¬†was a synonym for polygamy and the dowry system, both of which Rammohan campaigned against.¬†His father, Ramkanta, was a¬†Vaishnavite, while his mother, Tarini Devi, was from a¬†Shaivite¬†family. He was a great scholar of Sanskrit, Persian and English languages and also knew Arabic, Latin and Greek. One parent prepared him for the occupation of a scholar, the¬†Shastri, while the other secured for him all the worldly advantages needed to launch a career in the¬†laukik¬†or worldly sphere of public administration.[citation needed]¬†Torn between these two parental ideals from early childhood, Ram Mohan vacillated between the two for the rest of his life.During his childhood Ram Mohan Roy witnessed death of his sister in law through¬†sati. The seventeen year old girl was dragged towards the pyre where Ram Mohan Roy witnessed her terrified state. He tried to protest but to no avail. She was burned alive. The people chanted \"Maha Sati! Maha Sati! Maha Sati!\" (great wife) over her painful screams.¬†<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>Instruction:<br>What languages did Ram Mohan Roy know?<br><br>Context:<br>Ram Mohan Roy was born in¬†Radhanagar,¬†Hooghly District,¬†Bengal Presidency. His great grandfather Krishnakanta Bandyopadhyay was a Rarhi¬†Kulin¬†(noble)¬†Brahmin. Among Kulin Brahmins¬†‚Äì descendants of the six families of Brahmins imported from¬†Kannauj¬†by¬†Ballal Sen¬†in the 12th century¬†‚Äì those from the Rarhi district of West Bengal were notorious in the 19th century for living off dowries by marrying several women.¬†Kulinism¬†was a synonym for polygamy and the dowry system, both of which Rammohan campaigned against.¬†His father, Ramkanta, was a¬†Vaishnavite, while his mother, Tarini Devi, was from a¬†Shaivite¬†family. He was a great scholar of Sanskrit, Persian and English languages and also knew Arabic, Latin and Greek. One parent prepared him for the occupation of a scholar, the¬†Shastri, while the other secured for him all the worldly advantages needed to launch a career in the¬†laukik¬†or worldly sphere of public administration.[citation needed]¬†Torn</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}