{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bbd2dc-92fd-4ecf-8451-92cc2227717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  This notebook demonstrates the following:\n",
    "#  * fine tuning \"gemma2_2b_en\" on the dolly dataset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ec2212-9c95-4a99-aede-ae4f86d0b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python imports\n",
    "#\n",
    "#  Notes:\n",
    "#  * Make sure you install the packages in requirements.txt\n",
    "#  * Make sure you setup your KAGGLE secrets via env vars.\n",
    "#\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
    "from keras.models import load_model\n",
    "import kagglehub\n",
    "from langchain.schema.runnable import Runnable\n",
    "from typing import Any, Optional\n",
    "import tensorflow as tf\n",
    "from keras.config import disable_interactive_logging\n",
    "import gc\n",
    "from IPython.display import Markdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab0264d-bc3a-47e2-8ad2-fc91f9a63a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions we will use later to display the interaction with the model\n",
    "#\n",
    "def display_chat(prompt, text):\n",
    "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
    "  prompt = prompt.replace('\\n','<br>')\n",
    "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  text = textwrap.indent(text, '', predicate=lambda _: True)\n",
    "  text = text.replace('\\n\\n','<br><br>')\n",
    "  text = text.replace('\\n','<br>')\n",
    "  text = text.replace(\"```\",\"\")\n",
    "  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + text + \"</blockquote></font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)\n",
    "def to_markdown(text):\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b882bb0-8ba7-400a-9410-07adebddc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dolly dataset as needed\n",
    "!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbedc30a-3c59-47b0-9098-cf245afce06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dolly dataset \n",
    "#\n",
    "import json\n",
    "data = []\n",
    "with open(\"../../data/databricks-dolly-15k.jsonl\") as file: # Path will depend on your setup\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        # Filter out examples with context, to keep it simple.\n",
    "        if features[\"context\"]:\n",
    "            continue\n",
    "        # Format the entire example as a single string.\n",
    "        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "        data.append(template.format(**features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69498cf3-20af-4dd5-abe4-d0ac432ff73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on how much of the dataset to use for fine-tuning\n",
    "#\n",
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed12d69-cec3-4fca-90a1-5c57b7aa4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Load the default gemma model\n",
    "#\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85799e29-8acd-4da3-830a-e9c2791969fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727389413.273494 12995433 service.cc:146] XLA service 0x600000745400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727389413.273899 12995433 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1727389413.374243 12995433 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>What should I do on a trip to Europe?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote><br><br>[User 0001]<br><br>I'm going to Europe for the first time in a few months. I'm going to be in Paris, London, and Amsterdam. I'm not sure what to do. I'm not a big fan of museums, but I'm not sure what else to do. I'm not a big fan of shopping, but I'm not sure what else to do. I'm not a big fan of nightlife, but I'm not sure what else to do. I'm not a big fan of food, but I'm not sure what else to do. I'm not a big fan of sports, but I'm not sure what else to do. I'm not a big fan of music, but I'm not sure what else to do. I'm not a big fan of movies, but I'm not sure what else to do. I'm not a big fan of TV, but I'm not sure what else to do. I'm not a big fan of books, but I'm not sure what else to do. I'm not a big fan of video games, but I'm not sure what else to do. I'm not a big fan of computers, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to do. I'm not a big fan of buses, but I'm not sure what else to do. I'm not a big fan of cars, but I'm not sure what else to do. I'm not a big fan of motorcycles, but I'm not sure what else to do. I'm not a big fan of trains, but I'm not sure what else to do. I'm not a big fan of planes, but I'm not sure what else to do. I'm not a big fan of boats, but I'm not sure what else to</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a simple query\n",
    "#\n",
    "prompt = \"What should I do on a trip to Europe?\"\n",
    "completion = gemma_lm.generate(prompt)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e449d9-29eb-4ff7-9168-cab061ab5d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Instruction:<br>What should I do on a trip to Europe?<br><br>Response:<br></blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to the Eiffel Tower.<br><br>What should I do on a trip to Europe?<br><br>Response:<br>I think you should go to</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a simple query this time using a specific template per the documentation\n",
    "#\n",
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "completion = gemma_lm.generate(prompt)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f278c2-9f83-4356-a8cb-576100e2f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now enable the model for LORA-based fine-tuning\n",
    "#\n",
    "gemma_lm.backbone.enable_lora(rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855d1c6-56d4-4606-be21-12824dd6762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  841/10544\u001b[0m \u001b[32m‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m193:32:30\u001b[0m 72s/step - loss: 0.8475 - sparse_categorical_accuracy: 0.5350"
     ]
    }
   ],
   "source": [
    "# Perform the fine-tuning\n",
    "#\n",
    "\n",
    "# Limit the input sequence length to 256 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 256\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8212850-9613-489c-804f-c127a6329c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
