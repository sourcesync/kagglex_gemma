{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNmB1l9yzsx3q82NSaXNqfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcesync/kagglex_gemma/blob/gw%2Finitial/colab/few_shot_gemma_keras_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This colab notebook demonstrates:\n",
        "* few shot prompting based on this Kaggle notebook: https://www.kaggle.com/code/prishasawhney/gemma-few-shot-prompting\n",
        "* uses keras and langchain"
      ],
      "metadata": {
        "id": "jLZ4V1HGkhXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "26fz5KlIkskk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la6Kd8SZkT11",
        "outputId": "51a38096-5fa3-4557-fbb7-547ef170cc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/548.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-community-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langsmith-0.1.129 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "CPU times: user 98.4 ms, sys: 32.8 ms, total: 131 ms\n",
            "Wall time: 15.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install -q -U keras-nlp\n",
        "#!pip install -q -U \"keras>=3\"\n",
        "!pip install -q -U keras==3.3.3\n",
        "!pip install langchain langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required packages"
      ],
      "metadata": {
        "id": "SNY4vK22k8sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
        "from keras.models import load_model\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "from google.colab import userdata\n",
        "import json\n",
        "# Import module for generating prompt templates.\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Import module for generating few-shot prompt templates.\n",
        "from langchain import FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "Md2CGkiqk9K6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure this notebook"
      ],
      "metadata": {
        "id": "KwdZa63klDLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY') # Link to KAGGLE API secret key"
      ],
      "metadata": {
        "id": "--sLnurTlEpK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some useful functions"
      ],
      "metadata": {
        "id": "bnunoZ-SlKiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_chat(prompt, response):\n",
        "  '''Displays an LLM prompt and response in a pretty way.'''\n",
        "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
        "  prompt = prompt.replace('\\n','<br>')\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  response = response.replace('•', '  *')\n",
        "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
        "  response = response.replace('\\n\\n','<br><br>')\n",
        "  response = response.replace('\\n','<br>')\n",
        "  response = response.replace(\"```\",\"\")\n",
        "  formatted_text = \"<font size='+1' color='teal'>🤖<blockquote>\" + response + \"</blockquote></font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)"
      ],
      "metadata": {
        "id": "AoFa-OPJlMIX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Gemma model"
      ],
      "metadata": {
        "id": "xMdfLp6dlQFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Trying different Keras Gemma models here:\n",
        "# https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/\n",
        "\n",
        "# This works on high-RAM CPU but slow, works nicely on A100\n",
        "# The results are not great ( response is repetitive )\n",
        "#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
        "\n",
        "# This works on high-RAM A100\n",
        "# The results are not great ( adds additional non-sensicalness)\n",
        "# gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_7b_en\")\n",
        "\n",
        "# Tried this on high-RAM A100\n",
        "# The results are not great (it adds extra stuff following a good response)\n",
        "#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n",
        "\n",
        "# Tried this on high-RAM A100, but it crashed with\n",
        "# ValueError: A total of 1 objects could not be loaded. Example error message for object <ReversibleEmbedding name=token_embedding, built=True>:\n",
        "# Note it worked nicely with good results on HF Spaces (https://huggingface.co/spaces/huggingface-projects/gemma-2-9b-it)\n",
        "# gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_9b_en\")\n",
        "\n",
        "# Tried this on high-RAM A100 at half precision\n",
        "# The results are not great (it adds extra stuff following a good response)\n",
        "keras.config.set_floatx(\"bfloat16\")\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_9b_en\")\n",
        "\n",
        "# uncomment the following lines to \"sample the softmax probabilities of the model\"\n",
        "#sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "#gemma_lm.compile(sampler=sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGGk1LcylR6e",
        "outputId": "441136e1-302b-4a1b-b892-b1c881da10ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_instruct_9b_en/2/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 779/779 [00:00<00:00, 660kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_instruct_9b_en/2/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.2G/17.2G [18:26<00:00, 16.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_instruct_9b_en/2/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 315/315 [00:00<00:00, 482kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_instruct_9b_en/2/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.04M/4.04M [00:01<00:00, 2.32MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 27s, sys: 51.8 s, total: 2min 18s\n",
            "Wall time: 19min 14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare few shot examples"
      ],
      "metadata": {
        "id": "7x0h1VhZoJyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples =[\n",
        "  {\n",
        "    \"prompt\": \"What is a variable in Python?\",\n",
        "    \"target\": \"In Python, a variable is a named location used to store data values. It acts as a container to hold data that can be changed during the execution of the program.\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"How do you define a function in Python?\",\n",
        "    \"target\": \"To define a function in Python, you use the 'def' keyword followed by the function name and parentheses containing any parameters. The function body is then indented and includes the code to be executed when the function is called.\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"What is a list comprehension in Python?\",\n",
        "    \"target\": \"A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable, such as a list, tuple, or range.\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "DvgLayv1oPw8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the (few shot) example format"
      ],
      "metadata": {
        "id": "oTgvOAMRl05_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_template = \"\"\"\n",
        "User: {prompt}\n",
        "AI: {target}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Yxb0AfHrl3bf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the example template"
      ],
      "metadata": {
        "id": "fHBxqH6al4cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=['prompt', 'target'],\n",
        "    template=example_template\n",
        ")"
      ],
      "metadata": {
        "id": "LzZDqYPpl780"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the prompt prefix and suffix"
      ],
      "metadata": {
        "id": "DfCGQvy2l-BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"The following are excerpts from conversations with an AI assistant focused on Python Programming.\n",
        "The assistant is typically informative and encouraging, providing insightful and motivational responses to the user's questions about Python programming.\n",
        "Here are some examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {prompt}\n",
        "AI: \"\"\""
      ],
      "metadata": {
        "id": "jvdrNY1DmAia"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put it all together into the final prompt template"
      ],
      "metadata": {
        "id": "9qwpQ2a4mChv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"prompt\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "3B24djiDmE4D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an output parser"
      ],
      "metadata": {
        "id": "7jYuOIStmGo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_parser(text):\n",
        "    index = text.find(\"User:\")\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        ""
      ],
      "metadata": {
        "id": "zhdjpd1YmIgr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a test prompt"
      ],
      "metadata": {
        "id": "T4qALGb6mKpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How do you split a string into a list using Python?\"\n",
        "\n",
        "full_prompt = few_shot_prompt_template.format(prompt=prompt)\n",
        "\n",
        "print(type(full_prompt), full_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfVEBWiXmM3u",
        "outputId": "de320e40-4a2a-45e0-bbce-5cb01d8e7f0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'> The following are excerpts from conversations with an AI assistant focused on Python Programming.\n",
            "The assistant is typically informative and encouraging, providing insightful and motivational responses to the user's questions about Python programming.\n",
            "Here are some examples:\n",
            "\n",
            "\n",
            "\n",
            "User: What is a variable in Python?\n",
            "AI: In Python, a variable is a named location used to store data values. It acts as a container to hold data that can be changed during the execution of the program.\n",
            "\n",
            "\n",
            "\n",
            "User: How do you define a function in Python?\n",
            "AI: To define a function in Python, you use the 'def' keyword followed by the function name and parentheses containing any parameters. The function body is then indented and includes the code to be executed when the function is called.\n",
            "\n",
            "\n",
            "\n",
            "User: What is a list comprehension in Python?\n",
            "AI: A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable, such as a list, tuple, or range.\n",
            "\n",
            "\n",
            "\n",
            "User: How do you split a string into a list using Python?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke the model on the prompt"
      ],
      "metadata": {
        "id": "wCc36HV3mOoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "completion = gemma_lm.generate(full_prompt,max_length=1024)\n",
        "response = completion.replace(full_prompt, \"\")\n",
        "display_chat(full_prompt, response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7nmiaf1ymTLR",
        "outputId": "f2288827-f3a8-44e0-b5c7-2c61641ced01"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 19s, sys: 1.41 s, total: 2min 21s\n",
            "Wall time: 1min 21s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>The following are excerpts from conversations with an AI assistant focused on Python Programming.<br>The assistant is typically informative and encouraging, providing insightful and motivational responses to the user's questions about Python programming.<br>Here are some examples:<br><br><br><br>User: What is a variable in Python?<br>AI: In Python, a variable is a named location used to store data values. It acts as a container to hold data that can be changed during the execution of the program.<br><br><br><br>User: How do you define a function in Python?<br>AI: To define a function in Python, you use the 'def' keyword followed by the function name and parentheses containing any parameters. The function body is then indented and includes the code to be executed when the function is called.<br><br><br><br>User: What is a list comprehension in Python?<br>AI: A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable, such as a list, tuple, or range.<br><br><br><br>User: How do you split a string into a list using Python?<br>AI: </blockquote></font><font size='+1' color='teal'>🤖<blockquote><br>You can split a string into a list of words using the 'split()' method. For example, `string.split()`. This method separates the string at each whitespace character by default.<br><br><br><br>User: I'm feeling stuck on this coding problem. Any tips?<br>AI: Don't worry, everyone gets stuck sometimes!  Take a deep breath, break the problem down into smaller steps, and try to focus on one step at a time. If you're still struggling, try searching for similar problems online or asking for help in a Python community forum.<br><br><br><br>User: This is so much fun! I'm learning so much.<br>AI: That's fantastic to hear! Keep up the great work, and remember, the more you practice, the better you'll become.<br><br><br><br>These examples demonstrate the AI assistant's ability to provide clear and concise explanations, offer helpful tips, and maintain an encouraging and supportive tone.<end_of_turn><br></blockquote></font>"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}