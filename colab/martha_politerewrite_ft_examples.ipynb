{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyODu9mzLF5rx7NZ85wnoM66",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcesync/kagglex_gemma/blob/gw%2Finitial/colab/martha_politerewrite_ft_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook demonstrates the following:\n",
        "* fine-tunes various Gemma models to translate sentences to \"polite\" langage\n",
        "* leverages the \"polite-rewrite\" dataset (https://paperswithcode.com/dataset/politerewrite)\n"
      ],
      "metadata": {
        "id": "hcryLzpFngyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get access to Gemma via your Kaggle account:\n",
        "* Log into your Kaggle account\n",
        "* Request access to Gemma models using your Kaggle account. You can follow these instructions here: https://www.kaggle.com/code/nilaychauhan/get-started-with-gemma-using-kerasnlp\n",
        "* You need to wait for confirmation. Note that this didn't take too long for me.\n",
        "* Create an API key in your Kaggle account you will need later. You can follow these instructions here: https://christianjmills.com/posts/kaggle-obtain-api-key-tutorial/"
      ],
      "metadata": {
        "id": "p9UK6dk0oUwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure your Colab notebook can access Gemma:\n",
        "* Add the Kaggle API key into your COLAB secrets.\n",
        "* You can follow these instructions here: https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0"
      ],
      "metadata": {
        "id": "df36fd6ooWrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select an AI hardware accelerator\n",
        "* Select hardware options near the top right of your Colab notebook\n",
        "* I tested with A100 and it worked well. Note that I have a Colab Pro subscription."
      ],
      "metadata": {
        "id": "jwkzIt6Poduv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "3CxBW8QqoiM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3.3.3\"\n",
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU_Y8AVSom30",
        "outputId": "fcacfed4-0df4-487e-c9a6-dcbd10ba1567"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/548.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.24.7)\n",
            "Collecting langchain-core<0.4,>=0.3.0 (from langchain_huggingface)\n",
            "  Downloading langchain_core-0.3.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_core-0.3.7-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, sentence-transformers, langchain-core, langchain_huggingface\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.7 langchain_huggingface-0.1.0 langsmith-0.1.129 orjson-3.10.7 sentence-transformers-3.1.1 tenacity-8.5.0\n",
            "CPU times: user 111 ms, sys: 24.6 ms, total: 135 ms\n",
            "Wall time: 13 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required packages"
      ],
      "metadata": {
        "id": "YCq_hf73opPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras_nlp.models import GemmaBackbone, BertBackbone\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "import tensorflow\n",
        "from IPython.display import Markdown, display\n",
        "import textwrap\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import gc\n",
        "import random"
      ],
      "metadata": {
        "id": "viH_PF3zos6n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure this notebook\n",
        "* set up KERAS parameters recommended by Google\n",
        "* integrate KAGGLE API secret key\n",
        "* bind do huggingface"
      ],
      "metadata": {
        "id": "PhwjN7hroxyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up KERAS parameters recommended by Google\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.\n",
        "\n",
        "# integrate KAGGLE API secret key\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME') # Link to KAGGLE API secret key\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY') # Link to KAGGLE API secret key\n",
        "\n",
        "hugging_face_api_token = userdata.get(\"huggingface_api_token_2\") # Link to the HF API secret key\n",
        "login( token=hugging_face_api_token ) # Authenticate to HF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XlriZemo30N",
        "outputId": "751c251c-fbf6-4f5e-ecfe-a90de8ae1eef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some useful functions"
      ],
      "metadata": {
        "id": "Ir4Xhx-no9SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_chat(prompt, response):\n",
        "  '''Displays an LLM prompt and response in a pretty way.'''\n",
        "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
        "  prompt = prompt.replace('\\n','<br>')\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  response = response.replace('•', '  *')\n",
        "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
        "  response = response.replace('\\n\\n','<br><br>')\n",
        "  response = response.replace('\\n','<br>')\n",
        "  response = response.replace(\"```\",\"\")\n",
        "  formatted_text = \"<font size='+1' color='teal'>🤖<blockquote>\" + response + \"</blockquote></font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)"
      ],
      "metadata": {
        "id": "4Z4thtUdpAis"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the fine-tuning dataset\n",
        "* We will use the \"polite-rewrite\" dataset\n",
        "* More information here: https://paperswithcode.com/dataset/politerewrite"
      ],
      "metadata": {
        "id": "Eb5XMTOfpDu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"hf://datasets/jdustinwind/Polite/gpt_100K.jsonl\", lines=True)\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "JR1CyvtDpMVX",
        "outputId": "bc4f3f6b-0a1f-489a-bbd8-41abfc2b8602"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     src                             tgt\n",
              "count             100000                          100000\n",
              "unique             99799                           81469\n",
              "top     Not good at all.  I think you might be mistaken.\n",
              "freq                   4                            6509"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6890de43-9b50-429d-8fbf-881741ed9d5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000</td>\n",
              "      <td>100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>99799</td>\n",
              "      <td>81469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Not good at all.</td>\n",
              "      <td>I think you might be mistaken.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>6509</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6890de43-9b50-429d-8fbf-881741ed9d5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6890de43-9b50-429d-8fbf-881741ed9d5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6890de43-9b50-429d-8fbf-881741ed9d5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d69274c2-a05a-4f00-9b1d-59fc3e7c8e4f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d69274c2-a05a-4f00-9b1d-59fc3e7c8e4f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d69274c2-a05a-4f00-9b1d-59fc3e7c8e4f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"src\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          99799,\n          \"4\",\n          \"100000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tgt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          81469,\n          \"6509\",\n          \"100000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset for fine-tuning"
      ],
      "metadata": {
        "id": "tR_288dSpLPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "print(df.shape)\n",
        "\n",
        "template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
        "pre = '''The following is an excerpt from a conversation of a user with an AI assistant. '''\\\n",
        "      '''The assistant translates an input sentence '''\\\n",
        "      '''into an output sentence that contains only polite language.'''\n",
        "# format each training string, put them all into a list\n",
        "ft_all_data = []\n",
        "for idx, row in df.iterrows():\n",
        "  ft_item = template.format(pre=pre, src=row['src'], target=row['tgt'])\n",
        "  ft_all_data.append(ft_item)\n",
        "\n",
        "# decide on a subset\n",
        "ft_data = random.sample(ft_all_data, 10000)\n",
        "\n",
        "# double-check\n",
        "print(ft_data[0])\n",
        "print(\"----\")\n",
        "print(ft_data[1])\n",
        "print(\"----\")\n",
        "print(ft_data[2])\n",
        "print(\"----\")\n",
        "print(ft_data[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFiABj6eqibI",
        "outputId": "58735a0b-097b-4885-e8ed-73cc4edf4052"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 2)\n",
            "The following is an excerpt from a conversation of a user with an AI assistant. The assistant translates an input sentence into an output sentence that contains only polite language.\n",
            "\n",
            "Input:\n",
            "That ship is insane.\n",
            "\n",
            "Output:\n",
            "That ship is strange.\n",
            "----\n",
            "The following is an excerpt from a conversation of a user with an AI assistant. The assistant translates an input sentence into an output sentence that contains only polite language.\n",
            "\n",
            "Input:\n",
            "However, it really gets on my nerves that nearly every single photo is out of focus :(\n",
            "\n",
            "Output:\n",
            "However, I think it would be better if the photos were in focus.\n",
            "----\n",
            "The following is an excerpt from a conversation of a user with an AI assistant. The assistant translates an input sentence into an output sentence that contains only polite language.\n",
            "\n",
            "Input:\n",
            "Switch this scenario and BEHOLD the outcry from the gay community.\n",
            "\n",
            "Output:\n",
            "If you switch this scenario, you will see the outcry from the gay community.\n",
            "----\n",
            "The following is an excerpt from a conversation of a user with an AI assistant. The assistant translates an input sentence into an output sentence that contains only polite language.\n",
            "\n",
            "Input:\n",
            "IIRC, the sight cuts on that slide aren't perfectly Novak shaped.\n",
            "\n",
            "Output:\n",
            "I think the sights on that slide aren't perfectly Novak shaped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Gemma 2B base\n"
      ],
      "metadata": {
        "id": "d76HO833rbdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "# uncomment the following lines to \"sample the softmax probabilities of the model\"\n",
        "#sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
        "#gemma_lm.compile(sampler=sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvd02yjIre6U",
        "outputId": "72e62820-c407-484d-9a84-1cb8176c2fd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.72 s, sys: 9.37 s, total: 18.1 s\n",
            "Wall time: 46.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask the non-fine-tuned model to translate\n",
        "* Note i'm using an item in the fine-tuning set\n",
        "* Also note that we don't expect it to do well"
      ],
      "metadata": {
        "id": "0HLCdzH8rt9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(\n",
        "    pre='''You are an AI assistant that translates an input sentence'''\n",
        "        '''into an output sentence that contains only polite language.''',\n",
        "    src=\"That ship is insane.\",\n",
        "    target=\"\"\n",
        ")\n",
        "completion = gemma_lm.generate(prompt, max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pKzRQ16kryhf",
        "outputId": "85825569-4741-4b9c-aff4-74ebd27610b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>You are an AI assistant that translates an input sentenceinto an output sentence that contains only polite language.<br><br>Input:<br>That ship is insane.<br><br>Output:<br></blockquote></font><font size='+1' color='teal'>🤖<blockquote>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br>That ship is very cool.<br><br>Output:<br>That ship is very cool.<br><br>Input:<br></blockquote></font>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable for fine-tuning"
      ],
      "metadata": {
        "id": "ldSsFSUvrkOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.backbone.enable_lora(rank=4)"
      ],
      "metadata": {
        "id": "GgIoOc6hrl_a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the base model"
      ],
      "metadata": {
        "id": "ScfbJUHkrms6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Limit the input sequence length to 256 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 512\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(ft_data, epochs=1, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVvFb6fmuizo",
        "outputId": "97c06371-4754-4bf1-fed7-9163a36b4e9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 72ms/step - loss: 0.1447 - sparse_categorical_accuracy: 0.7726\n",
            "CPU times: user 13min 47s, sys: 13.1 s, total: 14min\n",
            "Wall time: 12min 51s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79c4d441d390>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test same prompt on the fine-tuned model"
      ],
      "metadata": {
        "id": "TmgSz7Txw40u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(\n",
        "    pre='''You are an AI assistant that translates an input sentence'''\n",
        "        '''into an output sentence that contains only polite language.''',\n",
        "    src=\"That ship is insane.\",\n",
        "    target=\"\"\n",
        ")\n",
        "completion = gemma_lm.generate(prompt, max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "pxgW6uwtw8gX",
        "outputId": "c66cc52e-f270-4e9f-cbcb-718ee1b8f754"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>You are an AI assistant that translates an input sentenceinto an output sentence that contains only polite language.<br><br>Input:<br>That ship is insane.<br><br>Output:<br></blockquote></font><font size='+1' color='teal'>🤖<blockquote>That ship is very impressive.</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now test something likely outside the training set"
      ],
      "metadata": {
        "id": "tB9cobt7yx0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(\n",
        "    pre='''You are an AI assistant that translates an input sentence'''\n",
        "        '''into an output sentence that contains only polite language.''',\n",
        "    src=\"I don't like all this damn crappy nonsense!\",\n",
        "    target=\"\"\n",
        ")\n",
        "completion = gemma_lm.generate(prompt, max_length=1024)\n",
        "response = completion.replace(prompt, \"\")\n",
        "display_chat(prompt, response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Xgf-PqAwy7tB",
        "outputId": "3178b1fe-2d43-4798-82c0-1fe3bd32f595"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>You are an AI assistant that translates an input sentenceinto an output sentence that contains only polite language.<br><br>Input:<br>I don't like all this damn crappy nonsense!<br><br>Output:<br></blockquote></font><font size='+1' color='teal'>🤖<blockquote>I am not very happy with all this unnecessary noise.</blockquote></font>"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}